{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_Analytics_Final_Project_for_Kyuhwan_Kim.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RfmhbCIM_m1w",
        "SxzvJgVi1Gri"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QOneK/Ryerson-Data-Analytics-Final-Project-for-Kyuhwan-Kim/blob/master/Data_Analytics_Final_Project_for_Kyuhwan_Kim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjjSNOqqryQR",
        "colab_type": "text"
      },
      "source": [
        "**Data Analytics Final Project for Kyuhwan Kim**\n",
        "\n",
        "---\n",
        "Dateset can be found at:\n",
        "https://www.kaggle.com/aaron7sun/stocknews\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgS8sof6tXH0",
        "colab_type": "text"
      },
      "source": [
        "# Importing files into Google Colabs\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLmyhqc_sEZP",
        "colab_type": "text"
      },
      "source": [
        "This code allows you to connect Google Colabs with Google Drive. This method was done so that the files do not need to be inputted everytime Colabs was run. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-06hbvJ4siBF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "0b36d0f0-a9b8-45fa-b46a-38f08f52f96b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek6SjmxStO8V",
        "colab_type": "text"
      },
      "source": [
        "** Use this portion of code if you cannot connect Google Drive and want to manually upload the dataset files**\n",
        "\n",
        "To use this portion of code, uncomment the # signs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhSB46yNrDvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqGn9ptvvKfx",
        "colab_type": "text"
      },
      "source": [
        "# Loading Python Libraries and CSV files, then running EDA (Exploratory Data Analysis)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsUoDbqCuohW",
        "colab_type": "text"
      },
      "source": [
        "Import the necessary libraries: Pandas, Numpy, Matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQjfSPunutRA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "b1b87e1b-b4f4-4e47-bf45-e6908307f1ab"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification,confusion_matrix,accuracy_score"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZtfHWiPx4Dr",
        "colab_type": "text"
      },
      "source": [
        "Read the .csv files and put them into variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZArDfVj8x_-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reddit = pd.read_csv('/content/drive/My Drive/Coding/GitHub for Kyuhwan Kim/Ryerson Data Analytics Final Project for Kyuhwan Kim/Dataset/RedditNews.csv')\n",
        "djia = pd.read_csv('/content/drive/My Drive/Coding/GitHub for Kyuhwan Kim/Ryerson Data Analytics Final Project for Kyuhwan Kim/Dataset/upload_DJIA_table.csv')\n",
        "reddit_djia = pd.read_csv('/content/drive/My Drive/Coding/GitHub for Kyuhwan Kim/Ryerson Data Analytics Final Project for Kyuhwan Kim/Dataset/Combined_News_DJIA.csv')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z911os0W-scj",
        "colab_type": "text"
      },
      "source": [
        "## EDA (Exploratory Data Analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfmhbCIM_m1w",
        "colab_type": "text"
      },
      "source": [
        "### EDA for Reddit Dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5Cy4KJ-qSLC",
        "colab_type": "text"
      },
      "source": [
        ".info() method allowed to see what the name of attributes, number of rows and the data types of the columns\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NigrALB4_u2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reddit.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eEQAVIYqm_M",
        "colab_type": "text"
      },
      "source": [
        "Used the .min() and .max() method to find the range of the dates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk9KZ4eE5-mu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to find the minimum date of reddit\n",
        "reddit['Date'].min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FQ2h7FY-lcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to find the maximum date of reddit\n",
        "reddit['Date'].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A4ZySX0quoM",
        "colab_type": "text"
      },
      "source": [
        ".nunique() method allowed to see how many unique dates and articles that were present"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbEJDHDBCSqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#number of unique inputs \n",
        "reddit.nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcFySJpb_2lG",
        "colab_type": "text"
      },
      "source": [
        "### EDA for DJIA Dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNgHZVXYq6RE",
        "colab_type": "text"
      },
      "source": [
        ".info() method allowed to see what the name of attributes, number of rows and the data types of the columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEPsrmjQ_1Jl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "djia.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3ofN9Uyq-Cd",
        "colab_type": "text"
      },
      "source": [
        "Used .min() and .max methods to find the range of the dates. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiZgKPFGASYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "djia['Date'].min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSbsBanyIBCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "djia['Date'].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlsvbMb5rLJa",
        "colab_type": "text"
      },
      "source": [
        ".describe() method allowed to display rudimentary statistics about the datasets. Based on experimentation, this works only with numeric datatypes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyZ06voxJ3ad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "djia.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tupcWfNlYzx8",
        "colab_type": "text"
      },
      "source": [
        "Visual Representation DIJA (Dow Jones Industrial Average) \"Adj Close\" Trends\n",
        "(It seems that there are up and down movements but a general downward trend with recovery at the end)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-yRANONY2D5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this takes some time to run\n",
        "plt.plot(djia['Date'],djia['Adj Close'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAizvbHSMvmV",
        "colab_type": "text"
      },
      "source": [
        "### EDA for reddit_djia Dataframe \n",
        "This is the \"cleaned\" dataset; combination of both news and DJIA (adj price) label. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ie202HXM1zx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reddit_djia.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx_zvRZUb2pT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reddit_djia.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAckvakHdia2",
        "colab_type": "text"
      },
      "source": [
        "How many of the rows are 1? 0? in the class label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-EwNJEBcHA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reddit_djia['Label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLG3isYhs2Vu",
        "colab_type": "text"
      },
      "source": [
        "There appears to be more 1 (up) than 0 (down). However, from the trendline from DJIA Adj Price, we can see that there is actually a downward trend. Perhaps the magnitude of the up/down wasn't taken into consideration. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNz6pXNLtGUK",
        "colab_type": "text"
      },
      "source": [
        "This is to check the date range. From observation, we can see that the date range of the final \"cleaned\" dataset matches that of DIJA dataset. There seems to be extra erraneous data at the news dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqz2ohkXQy79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reddit_djia['Date'].min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WEPfQleQ_jF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reddit_djia['Date'].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb8MjVI0aHNq",
        "colab_type": "text"
      },
      "source": [
        "Plot Pie graph of Label\n",
        "https://matplotlib.org/3.1.1/gallery/pie_and_polar_charts/pie_features.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPA1XNNGbOGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = 'DJIA Up/Maintain', 'DJIA Down'\n",
        "sizes= [1065/1989, 924/1989]\n",
        "\n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
        "        shadow=True, startangle=90)\n",
        "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxzvJgVi1Gri",
        "colab_type": "text"
      },
      "source": [
        "## Checking if there are null values "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI2fqO_g33c9",
        "colab_type": "text"
      },
      "source": [
        "Check if there are null values that needs to be dealt with before merging the data. We can see that there were no null values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEmngngd39Aq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#check null values for reddit\n",
        "reddit.isnull().values.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EdZMqWC4mlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#check null values for djia\n",
        "djia.isnull().values.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrAXfGzc5K_u",
        "colab_type": "text"
      },
      "source": [
        "# Combining Reddit articles with DJIA datasets to create \"cleaned, final\" dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1k3e0K1PGRi",
        "colab_type": "text"
      },
      "source": [
        "Result: \n",
        "cleaned final dataset named \"**result**\"\n",
        "\n",
        "The final cleaned dataset will be created using this procedure:\n",
        "\n",
        "a) create an extra column called 'Label' (which are the difference between proceeding day)\n",
        "\n",
        "b) using an if-statement, categorize:\n",
        "   1 (up) when DJIA increased or maintained the same\n",
        "   0 (down) when DJIA decreased\n",
        "\n",
        "c) organize the articles into the corresponding dates. This will involve grouping the articles into similar dates. \n",
        "\n",
        "d) merge the DJIA 'Label' column and the articles on 'Dates' column. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6S7oFWVLYro",
        "colab_type": "text"
      },
      "source": [
        "## Creating DJIA 'Label'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lV4mSS5Lx1J",
        "colab_type": "text"
      },
      "source": [
        "I have noticed that the dates of DJIA and Reddit datasets are in asc/desc order. For ease of cleaning, before the label is created, instead of desc, use asc order for the djia dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC_2r1u3JLmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://datatofish.com/sort-pandas-dataframe/\n",
        "djia.sort_values(by = ['Date'], inplace=True, ascending=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTIjT_oyMB2d",
        "colab_type": "text"
      },
      "source": [
        "Copy 'Adj Close' column and run .diff() method on the column to see the day by day changes in the DJIA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_BSvnCRJQZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "djia['Diff']= djia['Adj Close']\n",
        "\n",
        "#https://stackoverflow.com/questions/39479919/how-do-i-subtract-the-previous-row-from-the-current-row-in-a-pandas-dataframe-an\n",
        "djia['Diff']=djia.Diff.diff()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Quw5Ya9GJWye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This is run just in case there are N/A values. These will be filled with zeros. \n",
        "djia=djia.fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEiQIP9OMbTz",
        "colab_type": "text"
      },
      "source": [
        "This section will take the values of 'Diff' column and using if clauses, determine the value of the class variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHN6eqRoJx9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://guillim.github.io/pandas/2018/10/22/Pandas-if-else-on-columns.html\n",
        "\n",
        "conditions = [\n",
        "    (djia['Diff'] >= 0.0),\n",
        "    (djia['Diff'] < 0.0)]\n",
        "choices = [int(1),int(0)]\n",
        "\n",
        "djia['Label'] = np.select(conditions, choices, default='null')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPaEXBUZMtBO",
        "colab_type": "text"
      },
      "source": [
        "This section is to reset the index [count row from 0 onwards]. Prior to input of this code, the index would go in desc order from index 1989. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ojxk2lYJ16l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "djia.reset_index(drop=True, inplace=True)\n",
        "djia.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx3bkXR1NAJb",
        "colab_type": "text"
      },
      "source": [
        "The first label by default should be 0 even though Diff = 0.0\n",
        "This was manually added since the previous block of code didn't account for the exception of the first label value. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQy3Qni0J4HU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "djia.iloc[0,-1]=int(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsiHEho2J5yk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "djia.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K9RdUr2NZnx",
        "colab_type": "text"
      },
      "source": [
        "Prior to casting the Label column as integer, the column was an object. Changed since it is numerical value; more specifically integer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxfmphnRJ7N_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "djia['Label'] = djia['Label'].astype('int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5vIiuKJN5O8",
        "colab_type": "text"
      },
      "source": [
        "## Reorganizing Reddit News Dataset\n",
        "\n",
        "Initally, the dataset had two columns: Date, News\n",
        "\n",
        "The dataset was formated so that each row is organized in following fashion: \n",
        "Date, Top1 ... Top25 \n",
        "\n",
        "Basically, the news articles were compiled into one single date and outputted horizontally. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVAaU84YwFlP",
        "colab_type": "text"
      },
      "source": [
        "On the news dataset, this portion of code will organize the news articles into its corresponding dates using list of lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQQTPF8rvQGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This is for .at function\n",
        "#https://www.geeksforgeeks.org/python-pandas-dataframe-at/\n",
        "\n",
        "#declaration of variables\n",
        "previous_date = reddit.at[0, 'Date']\n",
        "newslist= []\n",
        "sub_newslist = []\n",
        "date_list = [previous_date]\n",
        "\n",
        "\"\"\"\n",
        "This 'for loop' will iterate from the beginning to the end of the list of dataset\n",
        "\n",
        "Each iteration of the loop, the Date column will go down. The date variable will \n",
        "be the new date value. Also, News will be stored into sub_newslist(temp list for storing articles in a single date)\n",
        "At the end of the loop, the \n",
        "\n",
        "The 'if statement' will be activated when the value of 'Date' changes. \n",
        "\n",
        "When the if statement is run:\n",
        "a) the date value will be inputted towards date_list list\n",
        "b) values collected in sub_newslist will be inputted into newslist list\n",
        "c) sub_newslist list (used as a temp collector) will be emptied \n",
        "\"\"\"\n",
        "\n",
        "for i in range(0,len(reddit['Date'])):\n",
        "  date = reddit.at[i,'Date']\n",
        "  if date != previous_date:\n",
        "    date_list.append(date)\n",
        "    newslist.append(sub_newslist)\n",
        "    sub_newslist = []\n",
        "  sub_newslist.append(reddit.at[i,'News'])\n",
        "  previous_date = date\n",
        "\n",
        "#last date needs a seperate code to add articles of final date into newslist list\n",
        "newslist.append(sub_newslist)\n",
        "\n",
        "#various print statements to check that the articles are organized correctly\n",
        "print(newslist[0][0])\n",
        "print(newslist[-1][-1])\n",
        "print(newslist[-1])\n",
        "print(len(newslist))\n",
        "print(len(date_list))\n",
        "print(date_list[0])\n",
        "print(date_list[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlmDRI8U7wP9",
        "colab_type": "text"
      },
      "source": [
        "This portion of code will take the list and output them into a correct dataframe format\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmgBFO_e8W8d",
        "colab_type": "text"
      },
      "source": [
        "This code will create the labels for Top 25 articles. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI508XVJUjv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "f is f string. allows for consistency in formatting\n",
        "\"\"\"\n",
        "top25_labels = [f'Top{str(integer)}' for integer in list(range(1,26))]\n",
        "top25_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YSoMKm38c2h",
        "colab_type": "text"
      },
      "source": [
        "This portion of code will output the joint database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K6IKyB5KDTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#variable declaration\n",
        "relevant_dates_with_news = []\n",
        "cols = []\n",
        "\n",
        "#loop will iterate from beginning to end of the dates (created from previous block of code)\n",
        "#each line of output from 'relevant_dates_with_news' will output the entire corresponding row\n",
        "#to a specific date \n",
        "\n",
        "for i in range(0,len(date_list)):\n",
        "  #date_with_news is a temporary list collector, that's why inside the loop\n",
        "  date_with_news = []\n",
        "  date_with_news.append(date_list[i])\n",
        "  date_with_news.extend(newslist[i])\n",
        "  relevant_dates_with_news.append(date_with_news)\n",
        "\n",
        "#This portion of code makes the dataframe with the header\n",
        "cols.append('Date')\n",
        "cols.extend(top25_labels)\n",
        "df = pd.DataFrame(columns=cols)\n",
        "\n",
        "#The loop inside loop, relevant_dates_with_news will iterate all the dates from \n",
        "#The break component ensures that each line has max of 25 articles. \n",
        "for j in range(0, len(relevant_dates_with_news)):\n",
        "  for k in range(0, len(relevant_dates_with_news[j])):\n",
        "    if k > 25:\n",
        "      break\n",
        "    df.at[j, cols[k]] = relevant_dates_with_news[j][k]\n",
        "\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCmuVgfZO1Xr",
        "colab_type": "text"
      },
      "source": [
        "## Combining the cleaned DJIA and Reddit datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz1KKwP6E1at",
        "colab_type": "text"
      },
      "source": [
        "The finalized databases are merged. The 'djia' is the base database where the newly created 'df' database will merge on 'Date' column. As of result,\n",
        "we expect the erraneous dates from the Reddit News will be automatically cropped\n",
        "if the date range goes beyond DJIA information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZK6sPWFcBPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_all = pd.merge(djia, df, how='inner', on=['Date'])\n",
        "result_all\n",
        "result = result_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdEGC7lOGoD6",
        "colab_type": "text"
      },
      "source": [
        "'result_all' will have all the necessary data. This can be used to do quantitative analysis as well. \n",
        "\n",
        "However, for classification, we can crop for only necessary "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ff5cuM9i456",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "CAUTION: Run this code ONLY once. Since, the drop code if run again will remove \n",
        "more columns than necessary\n",
        "\n",
        "If unappropriate result appears, run the previous block of code to reset \n",
        "result variable and run this portion again\n",
        "\n",
        "The result should have Date, Label, Top1 .. Top 25 columns\n",
        "\"\"\"\n",
        "result.drop(result.iloc[:, 1:8], axis=1, inplace=True)\n",
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG7OjpgGQHMY",
        "colab_type": "text"
      },
      "source": [
        "This portion of code tests whether or not each row has 25 articles. \n",
        "We find that there are some null values present. \n",
        "\n",
        "With this info, manually checked...\n",
        "\n",
        "**Though no columns had >25 articles, it was found that there were some dates with <25 articles.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-KChXXmKi1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "This portion of code found that BEFORE EVEN CLEANING DATA that there were some dates where it did not =25 articles\n",
        "\n",
        "Luckily, most can be neglected since when the dates are merged, most were taken out.\n",
        "\n",
        "However, it was later found that even after merging that some dates had <25 because of null values\n",
        "\"\"\"\n",
        "\n",
        "a = list(reddit['Date'])\n",
        "\n",
        "a,b = np.unique(a, return_counts=True)\n",
        "a[b != 25]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGXjDtQOVIpD",
        "colab_type": "text"
      },
      "source": [
        "This was a test to see where the NaN data can be found with the final dataset.\n",
        "\n",
        "The original dataset also had the NaN at the same locations. \n",
        "Therefore, the accuracy to replicate the finished clean data was a sucess. \n",
        "\n",
        "The missing data can be neglegible since there aren't many NaN\n",
        "and there are enough articles for even the non =25 article dates to run machine learning. \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwEbQnAWXDjV",
        "colab_type": "text"
      },
      "source": [
        "This portion of code finds the NaN values on 'result' dataframe. \n",
        "\n",
        "Even when the dataset is fully 'cleaned', there are some issues to be dealt with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIm_GdL4SmHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://dzone.com/articles/pandas-find-rows-where-columnfield-is-null\n",
        "null_columns=result.columns[result.isnull().any()]\n",
        "print(result[result.isnull().any(axis=1)][null_columns].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spBHsNyrViXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Dates where the missing data can be found. With more detail\n",
        "Scroll horizontally to the end to find some NaN values. \n",
        "\"\"\"\n",
        "\n",
        "#https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/\n",
        "\n",
        "result.iloc[[277,348,681]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2mV4gk6QprW",
        "colab_type": "text"
      },
      "source": [
        "Code to output the final cleaned data to file. \n",
        "\n",
        "When compared to final prepared data, the result was identical to the prepared data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3OpMeKSKrmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result.to_excel(\"cleaned_data.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znx67sKJJwnh",
        "colab_type": "text"
      },
      "source": [
        "# Natural Language Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXXAbaaTJ1c5",
        "colab_type": "text"
      },
      "source": [
        "Although the dataset has been cleaned and combined, there are still more work to be done. \n",
        "\n",
        "We still need to pre-process the data so that the machine learning algorithm can take the input. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSpFeeCFEHnh",
        "colab_type": "text"
      },
      "source": [
        "## Spliting Training and Testing Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVprdz2qI40X",
        "colab_type": "text"
      },
      "source": [
        "According to the dataset provider, there were instructions about how to split the training and test datsets. \n",
        "\n",
        "*\"For task evaluation, please use data from 2008-08-08 to 2014-12-31 as Training Set, and Test Set is then the following two years data (from 2015-01-02 to 2016-07-01). This is roughly a 80%/20% split.\"*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwGO2t1XJJFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Experimented with train = result['Date'] < '2015-01-01' but this gave a boolean result.\n",
        "Therefore, result[true values], the values that match the conditions were ouputted with final code.\n",
        "\"\"\"\n",
        "\n",
        "train = result[result['Date'] < '2015-01-01']\n",
        "test = result[result['Date'] > '2014-12-31']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoWxJTkaXlAI",
        "colab_type": "text"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1R-rpKes7Sb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "When looking at the list of news, we observe that some some reason, that the news articles \n",
        "start with a lower case 'b' character followed by ' or \". \n",
        "ex. b' or b\"\n",
        "\n",
        "Therefore, first step was to remove that b' or b\"\n",
        "\"\"\"\n",
        "\n",
        "result.replace(\"b'|b\\\"\", \" \", regex = True, inplace = True)\n",
        "result.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-BEsCila752",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "This selects just the news articles. This is where text feature engineering \n",
        "will be focused on\n",
        "\"\"\"\n",
        "data = train.iloc[:,2:27]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wM-oqMvPi8Jv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Removing punctuations\n",
        "\"\"\"\n",
        "data.replace(\"[^a-zA-Z]\",\" \", regex = True, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UZdMqsGljED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Converting to lower case characters\n",
        "\"\"\"\n",
        "\n",
        "for index in data:\n",
        "  data[index]=data[index].str.lower()\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vqvy80q1mqXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Combining each column of articles into one paragraph\n",
        "\"\"\"\n",
        "\n",
        "headline = []\n",
        "\n",
        "for row in range(0,len(data.index)):\n",
        "  headline.append(' '.join(str(x) for x in data.iloc[row,0:25]))\n",
        "\n",
        "headline[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Yi2mBlAo5fy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Implement Bag of Words\n",
        "\"\"\"\n",
        "countvector = CountVectorizer(ngram_range=(2,2))\n",
        "traindataset = countvector.fit_transform(headline)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9JTk98xp6HP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "RandomForest Classifier\n",
        "\"\"\"\n",
        "randomclassifier = RandomForestClassifier(n_estimators = 200, criterion = 'entropy')\n",
        "randomclassifier.fit(traindataset,train['Label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu09LoWTrvlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Predict for the Test Dataset\n",
        "\"\"\"\n",
        "test_transformed = []\n",
        "\n",
        "for row in range(0,len(test.index)):\n",
        "  test_transformed.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
        "\n",
        "test_dataset = countvector.transform(test_transformed)\n",
        "\n",
        "predictions = randomclassifier.predict(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnT5jdMjuoqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matrix = confusion_matrix(test['Label'],predictions)\n",
        "print (matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4xlMg08u8ue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = accuracy_score(test['Label'],predictions)\n",
        "print (score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-eVYotfvFzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "report=classification_report(test['Label'],predictions)\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}